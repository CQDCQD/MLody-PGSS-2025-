{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CQDCQD/MLody-PGSS-2025-/blob/main/MLody_Final_Version.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uejyvsMvH9pM"
      },
      "source": [
        "#Import libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jj1cu0PTH_4m"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "import numpy as np\n",
        "import librosa\n",
        "#audio processing library\n",
        "from librosa import load\n",
        "from scipy.signal import butter, filtfilt\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import os\n",
        "from torch.utils.data import Dataset\n",
        "from google.colab import auth\n",
        "from google.colab import drive\n",
        "import datetime\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "drive.mount('/content/drive', force_remount=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ousw9mY-HmJB"
      },
      "source": [
        "#Z score normalization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9KHWItO5_j8U"
      },
      "outputs": [],
      "source": [
        "def computeZScoreMeanStd(dataset):\n",
        "    melSum = 0.0\n",
        "    melSqSum = 0.0\n",
        "    stftSum = 0.0\n",
        "    stftSqSum = 0.0\n",
        "    totalElements = 0\n",
        "\n",
        "    loader = DataLoader(dataset, batch_size=1, shuffle=False)\n",
        "\n",
        "    for mel, stft, _, _ in loader:\n",
        "        mel = mel[0]  # [F, T]\n",
        "        stft = stft[0]\n",
        "\n",
        "        melSum += mel.sum().item()\n",
        "        melSqSum += (mel ** 2).sum().item()\n",
        "        stftSum += stft.sum().item()\n",
        "        stftSqSum += (stft ** 2).sum().item()\n",
        "        totalElements += mel.numel()\n",
        "\n",
        "    melMean = melSum / totalElements\n",
        "    melStd = np.sqrt((melSqSum / totalElements) - melMean**2)\n",
        "\n",
        "    stftMean = stftSum / totalElements\n",
        "    stftStd = np.sqrt((stftSqSum / totalElements) - stftMean**2)\n",
        "\n",
        "    return {\n",
        "        \"mel\": {\"mean\": melMean, \"std\": melStd},\n",
        "        \"stft\": {\"mean\": stftMean, \"std\": stftStd}\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rJtHySN-B231"
      },
      "source": [
        "#Globals"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "my-KMBKoH0ZV"
      },
      "outputs": [],
      "source": [
        "sr = 22050 #sample rate = 22050 hz\n",
        "file_name = \"SET FILE NAME\"\n",
        "local = True # Set to True if running locally, False if on a server\n",
        "modelSavePath = \"/content/PATH\" + datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\") + \".pth\"\n",
        "modelLoadPath = \"/content/PATH.pth\"\n",
        "loadDrive = \"/content/PATH/\"\n",
        "saveDrive = \"/content/PATH/\"\n",
        "file_path = loadDrive + file_name\n",
        "outputAPath = saveDrive + \"Split A/\"\n",
        "outputBPath = saveDrive + \"Split B/\"\n",
        "\n",
        "melPath   = \"/content/cached_data/mel_folder\"\n",
        "stftPath  = \"/content/cached_data/stft_folder\"\n",
        "maskPathA = \"/content/cached_data/mask_folderA\"\n",
        "maskPathB = \"/content/cached_data/mask_folderB\"\n",
        "\n",
        "calculatedZmelMean = 0.0027987720467882882\n",
        "calculatedZmelSTD = 0.020739795957941308\n",
        "calculatedZSTFTMean = 0.16094034763575416\n",
        "calculatedZSTFTSTD = 0.13678209049507903\n",
        "\n",
        "def getCalculatedZMelMean():\n",
        "    return calculatedZmelMean\n",
        "\n",
        "def getCalculatedZMelSTD():\n",
        "    return calculatedZmelSTD\n",
        "\n",
        "def getCalculatedZSTFTMean():\n",
        "    return calculatedZSTFTMean\n",
        "\n",
        "def getCalculatedZSTFTSTD():\n",
        "    return calculatedZSTFTSTD\n",
        "\n",
        "def getSavePath():\n",
        "    return saveDrive\n",
        "\n",
        "def getLoadPath():\n",
        "    return loadDrive\n",
        "\n",
        "def getOutputAPath():\n",
        "    return outputAPath\n",
        "\n",
        "def getOutputBPath():\n",
        "    return outputBPath\n",
        "\n",
        "def getSR():\n",
        "    return sr\n",
        "\n",
        "def getFilePath():\n",
        "    return file_path\n",
        "\n",
        "def getZScoreNormalization():\n",
        "    globalMean = globalZScoreNormalization.mean()\n",
        "    globalStd = globalZScoreNormalization.std()\n",
        "    return globalMean, globalStd\n",
        "\n",
        "def localMode():\n",
        "    return local\n",
        "\n",
        "def getModelSavePath():\n",
        "    return modelSavePath\n",
        "\n",
        "def getModelLoadPath():\n",
        "    return modelLoadPath"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a8gdxeV-H09U"
      },
      "source": [
        "#Import Audio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c_q6vlJRH7Bs"
      },
      "outputs": [],
      "source": [
        "def importAudio(file_path):\n",
        "    \"\"\"\n",
        "    Imports an audio file from the specified file path.\n",
        "\n",
        "    Args (arguments = inputs):\n",
        "        file_path (str): The path to the audio file to be imported.\n",
        "\n",
        "    Returns (what the function gives back when called):\n",
        "        audio: An AudioSegment object representing the audio file.\n",
        "        sr (int): The sample rate of the audio file.\n",
        "    \"\"\"\n",
        "    auth.authenticate_user()\n",
        "    drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "    audio, sr = load(file_path, sr=getSR(), mono=True)\n",
        "\n",
        "    return audio, sr\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-JbNVmpuH8qc"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YDLHTLbYIHbA"
      },
      "source": [
        "#Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ex4YQYPpISyx"
      },
      "outputs": [],
      "source": [
        "def preprocessAudio(audio):\n",
        "\n",
        "    audio = amplitudeNormalize(audio)\n",
        "    \"\"\"Normalize the audio amplitude to [0, 1] so it's in a consistent range.\n",
        "    If the music is louder, this will make it even to the model\"\"\"\n",
        "\n",
        "    audio = highPassFilter(audio, getSR())\n",
        "    \"\"\"Apply a high-pass filter to remove low-frequency noise. This removes\n",
        "    background sounds and isolates the vocals\"\"\"\n",
        "\n",
        "    audio = preEmphasis(audio)\n",
        "    \"\"\"Apply pre-emphasis to the audio signal. This boosts high frequencies\n",
        "    and helps the model focus on the important parts of the audio because\n",
        "    human hearing is more sensitive to these frequencies.\"\"\"\n",
        "\n",
        "    framedAudio = framing(audio, getSR())\n",
        "    \"\"\"Frame the audio signal into overlapping segments. This is done to\n",
        "    analyze the audio in smaller chunks, which is useful for processing\n",
        "    speech and music signals.\"\"\"\n",
        "\n",
        "    audioSTFT = STFTSpectrogram(framedAudio)\n",
        "    \"\"\"Compute the Short-Time Fourier Transform (STFT) of the audio signal.\n",
        "    This transforms the audio signal into the frequency domain, allowing us\n",
        "    to analyze its frequency content over time.\"\"\"\n",
        "\n",
        "    audioMel = MelSpectrogram(framedAudio, getSR())\n",
        "    \"\"\"Also compute the Mel spectrogram of the audio signal. This is a representation\n",
        "    of the audio signal that mimics human hearing by using a Mel scale, which\n",
        "    is more aligned with how humans perceive sound frequencies. We need this in addition\n",
        "    to the fourier transform because the fourier transform is better for small details\n",
        "    which can over-specify the model, so we need the mel spectrum for more generalized\n",
        "    application in conjunction with the specificity of the fourier transform\"\"\"\n",
        "\n",
        "    audioMel = logCompress(audioMel)\n",
        "    audioSTFT = logCompress(audioSTFT)\n",
        "    \"\"\"Apply logarithmic compression to the Mel spectrogram and STFT. This helps\n",
        "    to reduce the dynamic range of the audio signal, making it easier for the model to learn.\"\"\"\n",
        "\n",
        "    audioMel = globalZScale(audioMel, getCalculatedZMelMean(), getCalculatedZMelSTD())\n",
        "    audioSTFT = globalZScale(audioSTFT, getCalculatedZSTFTMean(), getCalculatedZSTFTSTD())\n",
        "    \"\"\"Apply global z-score normalization to the Mel spectrogram and STFT. This standardizes\n",
        "    the features by removing the mean and scaling to unit variance, which helps keep consistency\n",
        "    between samples.\"\"\"\n",
        "\n",
        "    return audioMel, audioSTFT\n",
        "    \"\"\"return the result\"\"\"\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "Functions used above ----------------------------------------------------------\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "def amplitudeNormalize(audio):\n",
        "    \"\"\"Librosa by default imports audio in form [-1, 1]\n",
        "    This function re-normalizes to [0, 1] for consistency\"\"\"\n",
        "    return (audio + 1) / 2\n",
        "\n",
        "def highPassFilter(audio, sr):\n",
        "    \"\"\"sets up a high-pass filter to remove low-frequency noise\"\"\"\n",
        "\n",
        "    cutoff = 75\n",
        "    \"\"\"hz, code cuts off frequencies below 75 hz\"\"\"\n",
        "    order = 4\n",
        "    \"\"\"order of the filter, higher order means sharper cutoff. 4 is a happy medium\n",
        "    between too sharp (choppy) and not sharp enough (doesn't cut well)\"\"\"\n",
        "    zeroPhaseFiltering = True\n",
        "    \"\"\"Makes the filter zero-phase, meaning it doesn't introduce\n",
        "    any phase distortion to the signal. This is important for audio processing to maintain\n",
        "    the original timing of the audio signal.\"\"\"\n",
        "    #butterworth filter\n",
        "    \"\"\"A butterworth filter is a type of signal processing filter that has a flat frequency response in the passband.\n",
        "    It is designed to have a smooth transition between the passband and the stopband, which\n",
        "    makes it suitable for audio processing applications where we want to remove low-frequency noise\n",
        "    without introducing significant phase distortion.\"\"\"\n",
        "\n",
        "\n",
        "    nyquist = 0.5 * sr\n",
        "    \"\"\"Nyquist frequency is half the sample rate, used to normalize the cutoff frequency\"\"\"\n",
        "    normal_cutoff = cutoff / nyquist\n",
        "    \"\"\"Normalize the cutoff frequency to the Nyquist frequency\"\"\"\n",
        "    \"\"\"This is done to ensure that the filter works correctly regardless of the sample rate\"\"\"\n",
        "\n",
        "    b, a = butter(order, normal_cutoff, btype='high', analog=False)\n",
        "    \"\"\"Create the Butterworth filter coefficients. The 'b' coefficients are the numerator and\n",
        "    'a' coefficients are the denominator of the filter transfer function. Unless you are interested,\n",
        "    ignore the fancy math and details\"\"\"\n",
        "\n",
        "    if zeroPhaseFiltering:\n",
        "        audio = filtfilt(b, a, audio)\n",
        "    \"\"\"Apply zero phase filtering to maintain signal phase\"\"\"\n",
        "\n",
        "    return audio\n",
        "    \"\"\"Return the filtered audio signal\"\"\"\n",
        "\n",
        "def preEmphasis(audio):\n",
        "\n",
        "    \"\"\"\n",
        "    Apply pre-emphasis to the audio signal.\n",
        "\n",
        "    Args:\n",
        "        audio (np.ndarray): The input audio signal.\n",
        "        alpha (float): The pre-emphasis coefficient.\n",
        "\n",
        "    Returns:\n",
        "        np.ndarray: The pre-emphasized audio signal.\n",
        "    \"\"\"\n",
        "\n",
        "    alpha = 0.97\n",
        "    \"\"\"Pre-emphasis coefficient, controls the amount of emphasis on high frequencies. The value 0.97 is\n",
        "    commonly used in speech processing because it provides a good balance between boosting high\n",
        "    frequencies and avoiding excessive noise amplification.\"\"\"\n",
        "\n",
        "    return np.append(audio[0], audio[1:] - alpha * audio[:-1])\n",
        "\n",
        "def framing(signal, sampleRate):\n",
        "    frameLen = 25\n",
        "    \"\"\"ms, length of each frame\"\"\"\n",
        "\n",
        "    desired_hop_samples = 256\n",
        "\n",
        "    frameLengthSamples = int(sampleRate * frameLen / 1000)\n",
        "\n",
        "    frameStepSamples = desired_hop_samples\n",
        "    \"\"\"Figures out sample sizes based on sample rates\"\"\"\n",
        "\n",
        "    resulting_overlap_samples = frameLengthSamples - frameStepSamples\n",
        "    resulting_overlap_ms = (resulting_overlap_samples / sampleRate) * 1000\n",
        "    print(f\"\\n--- Framing Details ---\")\n",
        "    print(f\"Desired frame length: {frameLen} ms ({frameLengthSamples} samples)\")\n",
        "    print(f\"Desired hop size: {frameStepSamples} samples\")\n",
        "    print(f\"Resulting overlap: {resulting_overlap_samples} samples ({resulting_overlap_ms:.2f} ms)\")\n",
        "\n",
        "    #check for short samples\n",
        "    if frameLengthSamples <= 0:\n",
        "        raise ValueError(f\"Calculated frameLengthSamples is {frameLengthSamples}. Ensure sampleRate and frameLen are positive.\")\n",
        "    # Ensure frameStepSamples is valid (non-negative and less than or equal to frameLengthSamples)\n",
        "    if frameStepSamples <= 0 or frameStepSamples > frameLengthSamples:\n",
        "        raise ValueError(f\"Calculated frameStepSamples is {frameStepSamples}. It must be positive and less than or equal to frameLengthSamples ({frameLengthSamples}).\")\n",
        "\n",
        "    if len(signal) < frameLengthSamples:\n",
        "        print(f\"Warning: Signal length ({len(signal)}) is less than frameLengthSamples ({frameLengthSamples}). No frames can be formed.\")\n",
        "        return np.array([])\n",
        "\n",
        "    numFrames = 1 + int((len(signal) - frameLengthSamples) / frameStepSamples)\n",
        "\n",
        "    if numFrames <= 0:\n",
        "        print(f\"Warning: numFrames is {numFrames}. Signal might be too short for the given frame parameters.\")\n",
        "        return np.array([])\n",
        "\n",
        "    shape = (numFrames, frameLengthSamples)\n",
        "    strides = (signal.strides[0] * frameStepSamples, signal.strides[0])\n",
        "    frames = np.lib.stride_tricks.as_strided(signal, shape=shape, strides=strides).copy()\n",
        "\n",
        "    windowType = 'hamming' # for smoothness\n",
        "    if windowType == 'hamming':\n",
        "        window = np.hamming(frameLengthSamples)\n",
        "    elif windowType == 'hann':\n",
        "        window = np.hanning(frameLengthSamples)\n",
        "    else:\n",
        "        raise ValueError(f\"Unsupported window type: {windowType}\")\n",
        "\n",
        "    return frames * window\n",
        "\n",
        "def STFTSpectrogram(frames_2d_array):\n",
        "    \"\"\"\n",
        "    Compute the Short-Time Fourier Transform (STFT) of a pre-framed audio signal.\n",
        "\n",
        "    Args:\n",
        "        frames_2d_array (np.ndarray): A 2D array where each row is a windowed frame\n",
        "                                      from the `framing` function.\n",
        "                                      Expected shape: (num_frames, frame_length_samples)\n",
        "\n",
        "    Returns:\n",
        "        np.ndarray: The magnitude spectrogram. Shape: (n_fft_bins, num_frames).\n",
        "                    Note: The output is magnitude (real-valued), not complex.\n",
        "    \"\"\"\n",
        "    if frames_2d_array.size == 0:\n",
        "        print(\"Warning: Input frames_2d_array is empty for STFTSpectrogram.\")\n",
        "        return np.array([[]]) # Return a 2D empty array\n",
        "\n",
        "    # Get the length of individual frames from the input 2D array\n",
        "    frame_length_samples = frames_2d_array.shape[1]\n",
        "\n",
        "    # --- SETTINGS FOR FFT ---\n",
        "    # nFFT: Number of points in the FFT. It's common to use the next power of 2\n",
        "    # greater than or equal to the frame_length_samples for FFT efficiency and zero-padding.\n",
        "    nFFT = int(2**np.ceil(np.log2(frame_length_samples)))\n",
        "\n",
        "    # Apply FFT to each frame (row)\n",
        "    # np.fft.rfft is for real-valued input and returns only the positive frequency components.\n",
        "    stft_complex = np.fft.rfft(frames_2d_array, n=nFFT, axis=1)\n",
        "\n",
        "    # Return magnitude spectrogram.\n",
        "    # Transpose (.T) to get the common (frequency_bins, time_frames) shape.\n",
        "    return np.abs(stft_complex).T\n",
        "\n",
        "\n",
        "\n",
        "def MelSpectrogram(frames_2d_array, sr):\n",
        "    \"\"\"\n",
        "    Compute the Mel spectrogram from a 2D array of pre-framed audio signals.\n",
        "    This function first computes the STFT of the frames and then applies\n",
        "    the Mel filter banks.\n",
        "\n",
        "    Args:\n",
        "        frames_2d_array (np.ndarray): A 2D array where each row is a windowed frame\n",
        "                                      from the `framing` function.\n",
        "                                      Expected shape: (num_frames, frame_length_samples)\n",
        "        sr (int): The sample rate.\n",
        "\n",
        "    Returns:\n",
        "        np.ndarray: The Mel spectrogram (power). Shape: (n_mels, num_frames).\n",
        "    \"\"\"\n",
        "    if frames_2d_array.size == 0:\n",
        "        print(\"Warning: Input frames_2d_array is empty for MelSpectrogram.\")\n",
        "        return np.array([[]]) # Return a 2D empty array\n",
        "\n",
        "    # === SETTINGS FOR MEL SPECTROGRAM ===\n",
        "    nMels = 128\n",
        "    \"\"\"Number of Mel frequency bins.\"\"\"\n",
        "    fMin = 60      # Hz\n",
        "    fMax = 7600    # Hz\n",
        "    \"\"\"Minimum and maximum frequencies for the Mel scale.\"\"\"\n",
        "\n",
        "    # Get the length of individual frames from the input 2D array\n",
        "    frame_length_samples = frames_2d_array.shape[1]\n",
        "\n",
        "    # nFFT for the internal FFT applied to frames (usually next power of 2)\n",
        "    nFFT_for_mel_internal = int(2**np.ceil(np.log2(frame_length_samples)))\n",
        "\n",
        "    # 1. Compute STFT (power) from the 2D frames using numpy's FFT\n",
        "    # This part is similar to the beginning of the STFTSpectrogram function\n",
        "    stft_complex = np.fft.rfft(frames_2d_array, n=nFFT_for_mel_internal, axis=1)\n",
        "    power_spectrogram = np.abs(stft_complex)**2 # Square magnitude for power spectrogram\n",
        "\n",
        "    # 2. Create Mel filter bank\n",
        "    # n_fft parameter for librosa.filters.mel should correspond to the FFT size used for the power_spectrogram\n",
        "    n_fft_for_mel_filter = (power_spectrogram.shape[1] - 1) * 2 # If power_spectrogram is (num_frames, n_fft_bins)\n",
        "                                                             # If power_spectrogram is (n_fft_bins, num_frames), use power_spectrogram.shape[0]\n",
        "\n",
        "    # Assuming power_spectrogram is (num_frames, n_fft_bins) from np.fft.rfft\n",
        "    # librosa.filters.mel expects n_fft, not n_fft_bins.\n",
        "    # The actual FFT length was nFFT_for_mel_internal.\n",
        "    mel_basis = librosa.filters.mel(\n",
        "        sr=sr,\n",
        "        n_fft=nFFT_for_mel_internal, # Use the nFFT that was used for the frames\n",
        "        n_mels=nMels,\n",
        "        fmin=fMin,\n",
        "        fmax=fMax,\n",
        "        htk=False\n",
        "    )\n",
        "\n",
        "    # 3. Apply Mel filter bank to the power spectrogram\n",
        "    # Power spectrogram is (num_frames, n_fft_bins), Mel basis is (n_mels, n_fft_bins)\n",
        "    # For matrix multiplication (np.dot), shapes need to align.\n",
        "    # np.dot(mel_basis, power_spectrogram.T) -> (n_mels, n_fft_bins) @ (n_fft_bins, num_frames)\n",
        "    melSpec = np.dot(mel_basis, power_spectrogram.T)\n",
        "\n",
        "    return melSpec\n",
        "\n",
        "\n",
        "def logCompress(spec):\n",
        "    \"\"\"\n",
        "    Apply logarithmic compression to the audio signal.\n",
        "\n",
        "    Args:\n",
        "        audio (np.ndarray): The input audio signal.\n",
        "\n",
        "    Returns:\n",
        "        np.ndarray: The log-compressed audio signal.\n",
        "\n",
        "    Add a small constant to avoid log(0) which is undefined\n",
        "\n",
        "    \"\"\"\n",
        "    return np.log1p(spec)  # log(1 + x) is numerically stable for small x\n",
        "\n",
        "def globalZScale(audio, mean, std):\n",
        "    return (audio - mean) / std\n",
        "    \"\"\"Apply global z-score normalization to the audio signal. This standardizes the features by removing the\n",
        "    mean and scaling to unit variance, which helps keep consistency between samples. The mean and std are\n",
        "    obtained from the globalZScoreNormalization module. Please check out globalZScoreNormalization.py\n",
        "    to see how the mean and std are computed.\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8w5cea64IUdg"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gquegZwjIT8J"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import os, random\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# U-Net block\n",
        "class UNetBlock(nn.Module):\n",
        "    def __init__(self, inChannels, outChannels):\n",
        "        super().__init__()\n",
        "        self.block = nn.Sequential(\n",
        "            nn.Conv2d(inChannels, outChannels, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(outChannels, outChannels, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.block(x)\n",
        "\n",
        "# Deep U-Net\n",
        "class DeepUNet(nn.Module):\n",
        "    def __init__(self, inputChannels=1, baseChannels=4):\n",
        "        super().__init__()\n",
        "        self.enc1 = UNetBlock(inputChannels, baseChannels)\n",
        "        self.pool1 = nn.MaxPool2d(2)\n",
        "        self.enc2 = UNetBlock(baseChannels, baseChannels * 2)\n",
        "        self.pool2 = nn.MaxPool2d(2)\n",
        "        self.bottleneck = UNetBlock(baseChannels * 2, baseChannels * 4)\n",
        "        self.up2 = nn.ConvTranspose2d(baseChannels * 4, baseChannels * 2, 2, stride=2)\n",
        "        self.dec2 = UNetBlock(baseChannels * 4, baseChannels * 2)\n",
        "        self.up1 = nn.ConvTranspose2d(baseChannels * 2, baseChannels, 2, stride=2)\n",
        "        self.dec1 = UNetBlock(baseChannels * 2, baseChannels)\n",
        "        self.final = nn.Conv2d(baseChannels, baseChannels, kernel_size=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x1 = self.enc1(x)\n",
        "        x2 = self.enc2(self.pool1(x1))\n",
        "        x3 = self.bottleneck(self.pool2(x2))\n",
        "        x = self.up2(x3)\n",
        "        x = self.dec2(torch.cat([x, F.interpolate(x2, size=x.shape[2:], mode='bilinear', align_corners=False)], dim=1))\n",
        "        x = self.up1(x)\n",
        "        x = self.dec1(torch.cat([x, F.interpolate(x1, size=x.shape[2:], mode='bilinear', align_corners=False)], dim=1))\n",
        "        return self.final(x)\n",
        "\n",
        "class DualUNetBLSTM(nn.Module):\n",
        "    def __init__(self, inputChannels=1, baseChannels=4, lstmHidden=64, melFreqBins=128, stftFreqBins=513):\n",
        "        super().__init__()\n",
        "        self.baseChannels = baseChannels\n",
        "        self.lstmHidden = lstmHidden\n",
        "        self.melFreqBins = melFreqBins\n",
        "        self.stftFreqBins = stftFreqBins\n",
        "\n",
        "        self.melUNet = DeepUNet(inputChannels, baseChannels)\n",
        "        self.stftUNet = DeepUNet(inputChannels, baseChannels)\n",
        "\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=2 * baseChannels * melFreqBins,\n",
        "            hidden_size=lstmHidden,\n",
        "            batch_first=True,\n",
        "            bidirectional=True\n",
        "        )\n",
        "        self.outputConv = nn.Conv2d(lstmHidden * 2, 2 * stftFreqBins, kernel_size=1)\n",
        "\n",
        "    def forward(self, melInput, stftInput):\n",
        "        melFeat = self.melUNet(melInput)\n",
        "        stftFeat = self.stftUNet(stftInput)\n",
        "\n",
        "        if stftFeat.shape[2:] != melFeat.shape[2:]:\n",
        "            stftFeat = F.interpolate(stftFeat, size=melFeat.shape[2:], mode='bilinear', align_corners=False)\n",
        "\n",
        "        fused = torch.cat([melFeat, stftFeat], dim=1)\n",
        "        b, c, f, t = fused.shape\n",
        "        rnnInput = fused.permute(0, 3, 1, 2).reshape(b, t, -1)\n",
        "\n",
        "        rnnOutput, _ = self.lstm(rnnInput)\n",
        "        rnnOutput = rnnOutput.permute(0, 2, 1).unsqueeze(2)  # [B, hidden*2, 1, T]\n",
        "\n",
        "        output = self.outputConv(rnnOutput)  # [B, 2*512, 1, T]\n",
        "        output = output.view(b, 2, self.stftFreqBins, t)  # Final: [B, 2, 512, T]\n",
        "\n",
        "        output = torch.sigmoid(output)\n",
        "        return output\n",
        "\n",
        "\n",
        "def predictMasks(melInput, stftInput):\n",
        "    device = torch.device(\"cuda\")\n",
        "\n",
        "    if isinstance(melInput, np.ndarray):\n",
        "        melInput = torch.tensor(melInput, dtype=torch.float32)\n",
        "    if isinstance(stftInput, np.ndarray):\n",
        "        stftInput = torch.tensor(stftInput, dtype=torch.float32)\n",
        "\n",
        "    if melInput.ndim == 2:\n",
        "        melInput = melInput.unsqueeze(0).unsqueeze(0)\n",
        "    elif melInput.ndim == 3:\n",
        "        melInput = melInput.unsqueeze(1)\n",
        "\n",
        "    if stftInput.ndim == 2:\n",
        "        stftInput = stftInput.unsqueeze(0).unsqueeze(0)\n",
        "    elif stftInput.ndim == 3:\n",
        "        stftInput = stftInput.unsqueeze(1)\n",
        "\n",
        "    melInput, stftInput = melInput.to(device), stftInput.to(device)\n",
        "\n",
        "    model = DualUNetBLSTM()\n",
        "    model.load_state_dict(torch.load(getModelLoadPath(), map_location=device))\n",
        "    model.to(device).eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        output = model(melInput, stftInput)\n",
        "\n",
        "    return output[0, 0].cpu(), output[0, 1].cpu()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vI6Ocs9sI33O"
      },
      "source": [
        "#Output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aLTrYwh7I2p-"
      },
      "outputs": [],
      "source": [
        "def reconstructSingleVoice(originalAudioPath, predictedMask, sampleRate=getSR()):\n",
        "    predictedMask = predictedMask ** 2\n",
        "\n",
        "    # --- 1. Load audio ---\n",
        "    signal, sr = librosa.load(originalAudioPath, sr=sampleRate)\n",
        "\n",
        "    # --- 2. Normalize amplitude to [0, 1] ---\n",
        "    signal = (signal + 1) / 2\n",
        "\n",
        "    # --- 3. High-pass filter (75 Hz) ---\n",
        "    nyquist = 0.5 * sr\n",
        "    cutoff = 75\n",
        "    b, a = butter(4, cutoff / nyquist, btype='high')\n",
        "    signal = filtfilt(b, a, signal)\n",
        "\n",
        "    # --- 4. Pre-emphasis ---\n",
        "    alpha = 0.97\n",
        "    emphasized = np.append(signal[0], signal[1:] - alpha * signal[:-1])\n",
        "\n",
        "    # --- 5. STFT ---\n",
        "    stft = librosa.stft(emphasized, n_fft=1024, hop_length=256, win_length=1024, window='hamming')\n",
        "    stftMag = np.abs(stft)\n",
        "    stftPhase = np.angle(stft)\n",
        "    stftMagLog = np.log1p(stftMag)\n",
        "\n",
        "    # --- 6. Resize predicted mask to match STFT shape ---\n",
        "    # predictedMask is [128, T], stftMagLog is [513, T]\n",
        "    predictedMask_resized = librosa.util.fix_length(predictedMask, size=stftMagLog.shape[1], axis=1)\n",
        "    predictedMask_resized = librosa.util.fix_length(predictedMask_resized, size=stftMagLog.shape[0], axis=0)\n",
        "\n",
        "    # --- 7. Apply mask ---\n",
        "    maskedLogMag = predictedMask_resized * stftMagLog\n",
        "    maskedMag = np.expm1(maskedLogMag)\n",
        "\n",
        "    # --- 8. Use Griffin-Lim to reconstruct waveform ---\n",
        "    reconstructed = librosa.griffinlim(\n",
        "        maskedMag,\n",
        "        n_iter=64,\n",
        "        hop_length=256,\n",
        "        win_length=1024,\n",
        "        window='hamming'\n",
        "    )\n",
        "\n",
        "    # --- 9. Undo pre-emphasis ---\n",
        "    for i in range(1, len(reconstructed)):\n",
        "        reconstructed[i] += alpha * reconstructed[i - 1]\n",
        "\n",
        "    # --- 10. re-normalize (is clipping) ---\n",
        "    max_val = np.max(np.abs(reconstructed))\n",
        "    if max_val > 1.0:\n",
        "        reconstructed = reconstructed / max_val * 0.95\n",
        "\n",
        "\n",
        "    # --- 11. Undo pre-emphasis ---\n",
        "    for i in range(1, len(reconstructed)):\n",
        "        reconstructed[i] += alpha * reconstructed[i - 1]\n",
        "\n",
        "    return reconstructed\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vYu4BqWKI99Y"
      },
      "source": [
        "#Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fWIjMMMhI_c4"
      },
      "outputs": [],
      "source": [
        "# === Dataset Class ===\n",
        "class SpectrogramDataset(Dataset):\n",
        "    def __init__(self, melFolder, stftFolder, maskFolderA, maskFolderB, max_T=320):\n",
        "        self.melFiles = sorted([os.path.join(melFolder, f) for f in os.listdir(melFolder) if f.endswith(\".npy\")])\n",
        "        self.stftFiles = sorted([os.path.join(stftFolder, f) for f in os.listdir(stftFolder) if f.endswith(\".npy\")])\n",
        "        self.maskFilesA = sorted([os.path.join(maskFolderA, f) for f in os.listdir(maskFolderA) if f.endswith(\".npy\")])\n",
        "        self.maskFilesB = sorted([os.path.join(maskFolderB, f) for f in os.listdir(maskFolderB) if f.endswith(\".npy\")])\n",
        "        self.max_T = max_T\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.melFiles)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        try:\n",
        "            # Load all spectrograms\n",
        "            mel = torch.tensor(np.load(self.melFiles[idx]), dtype=torch.float32).unsqueeze(0)     # [1, 512, T]\n",
        "            stft = torch.tensor(np.load(self.stftFiles[idx]), dtype=torch.float32).unsqueeze(0)   # [1, 512, T]\n",
        "            maskA = torch.tensor(np.load(self.maskFilesA[idx]), dtype=torch.float32).unsqueeze(0) # [1, 512, T]\n",
        "            maskB = torch.tensor(np.load(self.maskFilesB[idx]), dtype=torch.float32).unsqueeze(0) # [1, 512, T]\n",
        "\n",
        "            #print(f\"[{idx}] mel: {mel.shape}, stft: {stft.shape}, maskA: {maskA.shape}, maskB: {maskB.shape}\")\n",
        "\n",
        "            # Frequency dimension checks\n",
        "            if mel.shape[1] != 128:\n",
        "                print(f\"‚ö†Ô∏è Mel freq != 513 at {idx}, skipping.\")\n",
        "                return None\n",
        "            if any(x.shape[1] != 513 for x in [stft, maskA, maskB]):\n",
        "                print(f\"‚ö†Ô∏è STFT/mask freq != 513 at {idx}, skipping.\")\n",
        "                return None\n",
        "\n",
        "            T = mel.shape[2]\n",
        "\n",
        "            min_required_T = 20  # avoid tiny garbage tensors\n",
        "\n",
        "            if mel.shape[2] < min_required_T or stft.shape[2] < min_required_T \\\n",
        "              or maskA.shape[2] < min_required_T or maskB.shape[2] < min_required_T:\n",
        "                print(f\"‚ö†Ô∏è Sample {idx} is too short: mel={mel.shape[2]}, stft={stft.shape[2]}\")\n",
        "                return None\n",
        "\n",
        "\n",
        "            if T < self.max_T:\n",
        "                pad_amt = self.max_T - T\n",
        "                mel = F.pad(mel, (0, pad_amt))\n",
        "                stft = F.pad(stft, (0, pad_amt))\n",
        "                maskA = F.pad(maskA, (0, pad_amt))\n",
        "                maskB = F.pad(maskB, (0, pad_amt))\n",
        "\n",
        "            elif T > self.max_T:\n",
        "                start = random.randint(0, T - self.max_T)\n",
        "                mel = mel[:, :, start:start+self.max_T]\n",
        "                stft = stft[:, :, start:start+self.max_T]\n",
        "                maskA = maskA[:, :, start:start+self.max_T]\n",
        "                maskB = maskB[:, :, start:start+self.max_T]\n",
        "\n",
        "            min_T = min(mel.shape[-1], stft.shape[-1], maskA.shape[-1], maskB.shape[-1])\n",
        "            mel = mel[:, :, :min_T]\n",
        "            stft = stft[:, :, :min_T]\n",
        "            maskA = maskA[:, :, :min_T]\n",
        "            maskB = maskB[:, :, :min_T]\n",
        "\n",
        "\n",
        "            assert mel.shape[2] == self.max_T, f\"mel shape = {mel.shape}\"\n",
        "            assert stft.shape[2] == self.max_T, f\"stft shape = {stft.shape}\"\n",
        "            assert maskA.shape[2] == self.max_T, f\"maskA shape = {maskA.shape}\"\n",
        "            assert maskB.shape[2] == self.max_T, f\"maskB shape = {maskB.shape}\"\n",
        "\n",
        "\n",
        "            return mel, stft, maskA, maskB\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è Skipping file {idx}: {e}\")\n",
        "            return None\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# === Collate Function ===\n",
        "def collate_fn(batch):\n",
        "    batch = [b for b in batch if b is not None]  # remove invalid entries\n",
        "    if len(batch) == 0:\n",
        "        return None  # skip this batch entirely\n",
        "    mel, stft, maskA, maskB = zip(*batch)\n",
        "    return torch.stack(mel), torch.stack(stft), torch.stack(maskA), torch.stack(maskB)\n",
        "\n",
        "\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# === Dice Loss ===\n",
        "def dice_loss(pred, target, epsilon=1e-6):\n",
        "    intersection = (pred * target).sum(dim=(1, 2, 3))\n",
        "    union = pred.sum(dim=(1, 2, 3)) + target.sum(dim=(1, 2, 3))\n",
        "    dice = (2 * intersection + epsilon) / (union + epsilon)\n",
        "    return 1 - dice.mean()\n",
        "\n",
        "# === Full Loss Function ===\n",
        "def compute_total_loss(predA, predB, maskA, maskB, stft, epoch, criterion):\n",
        "    reconA = predA * stft\n",
        "    reconB = predB * stft\n",
        "    recon_loss = criterion(reconA, maskA * stft) + criterion(reconB, maskB * stft)\n",
        "\n",
        "    bce_loss = F.binary_cross_entropy(predA, maskA) + F.binary_cross_entropy(predB, maskB)\n",
        "    dice = dice_loss(predA, maskA) + dice_loss(predB, maskB)\n",
        "    mask_loss = 0.5 * bce_loss + 0.5 * dice\n",
        "\n",
        "    complementarity = torch.mean(torch.abs(predA + predB - 1.0))\n",
        "\n",
        "    k = 200\n",
        "    certainty_penalty = torch.mean(torch.exp(-((predA - 0.5) ** 2) * k)) + \\\n",
        "                        torch.mean(torch.exp(-((predB - 0.5) ** 2) * k))\n",
        "\n",
        "    confidence_gain = torch.mean(torch.abs(predA - 0.5)) + torch.mean(torch.abs(predB - 0.5))\n",
        "\n",
        "    entropyA = - (predA * torch.log(predA + 1e-6) + (1 - predA) * torch.log(1 - predA + 1e-6)).mean()\n",
        "    entropyB = - (predB * torch.log(predB + 1e-6) + (1 - predB) * torch.log(1 - predB + 1e-6)).mean()\n",
        "\n",
        "    # Weights\n",
        "    certainty_weight = 0.7\n",
        "    confidence_weight = 0.25\n",
        "    entropy_weight = 0.05\n",
        "    complementarity_weight = 0.05\n",
        "\n",
        "    total_loss = (\n",
        "        recon_loss\n",
        "        + mask_loss\n",
        "        + complementarity_weight * complementarity\n",
        "        + certainty_weight * certainty_penalty\n",
        "        - confidence_weight * confidence_gain\n",
        "        + entropy_weight * (entropyA + entropyB)\n",
        "    )\n",
        "    return total_loss\n",
        "\n",
        "# === Training Loop ===\n",
        "def trainModel(model, dataloader, numEpochs, lr, device='cuda', patience=12):\n",
        "    import copy\n",
        "    model.to(device)\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
        "    criterion = torch.nn.MSELoss()\n",
        "\n",
        "    best_loss = float('inf')\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    epochs_no_improve = 0\n",
        "\n",
        "    scaler = torch.cuda.amp.GradScaler()\n",
        "\n",
        "    for epoch in range(numEpochs):\n",
        "        model.train()\n",
        "        totalLoss = 0\n",
        "\n",
        "        for batch in dataloader:\n",
        "            if batch is None:\n",
        "                continue\n",
        "\n",
        "            mel, stft, maskA, maskB = batch\n",
        "\n",
        "            try:\n",
        "                mel, stft, maskA, maskB = mel.to(device), stft.to(device), maskA.to(device), maskB.to(device)\n",
        "\n",
        "                with torch.cuda.amp.autocast():\n",
        "                    output = model(mel, stft)\n",
        "                    predA = output[:, 0].unsqueeze(1)\n",
        "                    predB = output[:, 1].unsqueeze(1)\n",
        "\n",
        "                    loss = compute_total_loss(predA, predB, maskA, maskB, stft, epoch, criterion)\n",
        "\n",
        "                optimizer.zero_grad()\n",
        "                scaler.scale(loss).backward()\n",
        "                scaler.step(optimizer)\n",
        "                scaler.update()\n",
        "\n",
        "                totalLoss += loss.detach().item()\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"‚ö†Ô∏è Skipping batch due to error: {e}\")\n",
        "                continue\n",
        "\n",
        "        avgLoss = totalLoss / len(dataloader)\n",
        "\n",
        "        # === Logging ===\n",
        "        print(f\"Epoch {epoch+1}/{numEpochs}\")\n",
        "        print(\"Loss:\", avgLoss)\n",
        "        print(\"predA mean:\", predA.mean().item(), \"std:\", predA.std().item())\n",
        "        print(\"MSE loss:\", criterion(predA, maskA).item())\n",
        "        print(\"maskA vs maskB diff:\", torch.mean(torch.abs(maskA - maskB)).item())\n",
        "\n",
        "        # === Early stopping ===\n",
        "        if avgLoss < best_loss - 1e-5:\n",
        "            best_loss = avgLoss\n",
        "            best_model_wts = copy.deepcopy(model.state_dict())\n",
        "            epochs_no_improve = 0\n",
        "        else:\n",
        "            epochs_no_improve += 1\n",
        "            if epochs_no_improve >= patience:\n",
        "                print(f\"‚èπÔ∏è Early stopping at epoch {epoch+1}\")\n",
        "                break\n",
        "\n",
        "    model.load_state_dict(best_model_wts)\n",
        "    torch.save(model.state_dict(), getModelSavePath())\n",
        "    print(\"‚úÖ Best model saved.\")\n",
        "\n",
        "\n",
        "# === Run Training ===\n",
        "def runTraining():\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(f\"üöÄ Running on: {device}\")\n",
        "\n",
        "    # ‚úÖ Model to GPU\n",
        "    model = DualUNetBLSTM().to(device)\n",
        "\n",
        "    # ‚úÖ Dataset + DataLoader\n",
        "    dataset = SpectrogramDataset(melPath, stftPath, maskPathA, maskPathB)\n",
        "\n",
        "    # ‚úÖ Recommended batch size for A100: 8, 16, or even 32\n",
        "    loader = DataLoader(\n",
        "        dataset,\n",
        "        batch_size=128,  # adjust upward if you have enough memory\n",
        "        shuffle=True,\n",
        "        collate_fn=collate_fn,\n",
        "        num_workers=4,  # increase for faster loading\n",
        "        pin_memory=True\n",
        "    )\n",
        "\n",
        "    # Run the actual training loop\n",
        "    trainModel(model, loader, numEpochs=150, lr=1e-4, device=device)\n",
        "\n",
        "\n",
        "# === Inference Helper ===\n",
        "def predictMasks(melInput, stftInput):\n",
        "    device = torch.device(\"cuda\")\n",
        "\n",
        "    if isinstance(melInput, np.ndarray):\n",
        "        melInput = torch.tensor(melInput, dtype=torch.float32)\n",
        "    if isinstance(stftInput, np.ndarray):\n",
        "        stftInput = torch.tensor(stftInput, dtype=torch.float32)\n",
        "\n",
        "    if melInput.ndim == 2:\n",
        "        melInput = melInput.unsqueeze(0).unsqueeze(0)\n",
        "    elif melInput.ndim == 3:\n",
        "        melInput = melInput.unsqueeze(1)\n",
        "\n",
        "    if stftInput.ndim == 2:\n",
        "        stftInput = stftInput.unsqueeze(0).unsqueeze(0)\n",
        "    elif stftInput.ndim == 3:\n",
        "        stftInput = stftInput.unsqueeze(1)\n",
        "\n",
        "    melInput, stftInput = melInput.to(device), stftInput.to(device)\n",
        "\n",
        "    model = DualUNetBLSTM()\n",
        "    model.load_state_dict(torch.load(getModelLoadPath(), map_location=device))\n",
        "    model.to(device).eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        output = model(melInput, stftInput)\n",
        "\n",
        "    return output[0, 0].cpu(), output[0, 1].cpu()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oiFGXF2EBm1S"
      },
      "source": [
        "#Main\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3sFF1MIxBoDt"
      },
      "outputs": [],
      "source": [
        "\"\"\"Now on to where the code actually does stuff\"\"\"\n",
        "\n",
        "audio = importAudio(getFilePath())\n",
        "\"\"\"This function imports the audio from a file (whose name is included in globals).\n",
        "We use a function call instead of directly accessing the variable as good practice\n",
        "so we can't accidently change the variable\n",
        "\n",
        "Now please view the file \"import_audio.py\" to see how this function works (if you want)\"\"\"\n",
        "\n",
        "\n",
        "\"\"\"Now that we know how the audio is imported, we can preprocess it to get it ready\n",
        "for the model. We need to do this to mimic how humans hear music to increase the\n",
        "ability of the model to separate voices like a human would\"\"\"\n",
        "\n",
        "globalZScoreNormalization = init(dataset = None) # Need to give dataset ---------------------------\n",
        "\"\"\"gets some data used in preprocessing (please ignore for now)\"\"\"\n",
        "audioMel, audioSTFT = preprocessAudio(audio)\n",
        "\"\"\"This function does several steps of preprocessing. Please view the file\n",
        "\"preprocessing.py\" to see how this function works (if you want)\"\"\"\n",
        "\n",
        "\n",
        "maskA, maskB = predictMasks(audioMel, audioSTFT)\n",
        "\"\"\"Run the neural network model to figure out the masks for each voice used to isolate it.\n",
        "Please view model.py to see how this function works\"\"\"\n",
        "\n",
        "outputA = reconstructSingleVoice(getFilePath(), maskA)\n",
        "outputB = reconstructSingleVoice(getFilePath(), maskB)\n",
        "\n",
        "\"\"\"Reconstruct the audio for each voice using the masks.\"\"\"\n",
        "\n",
        "os.save(outputA, getOutputPathA() + str(datetime.datetime.now()))\n",
        "os.save(outputB, getOutputPathB() + str(datetime.datetime.now()))\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LlxOz3MBWY-w"
      },
      "source": [
        "#Blank Model Test Code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SK6D-xoEc6BP"
      },
      "source": [
        "TODO list\n",
        "~~audio import~~\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ky1Qsk5ablzA"
      },
      "source": [
        "Test audio import"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BVLxuIwhWa4E"
      },
      "outputs": [],
      "source": [
        "audio = importAudio(getFilePath())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0G45c8z4bdVC"
      },
      "source": [
        "Check file path is valid"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "72effc2f"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from google.colab import drive\n",
        "\n",
        "# Make sure drive is mounted\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "file_path = getFilePath() # Get the file path from globals\n",
        "\n",
        "# Check if the file exists\n",
        "if os.path.exists(file_path):\n",
        "    print(f\"Success: The file exists at {file_path}\")\n",
        "else:\n",
        "    print(f\"Error: The file was NOT found at {file_path}\")\n",
        "    # Let's try to list the contents of the parent directory\n",
        "    parent_dir = os.path.dirname(file_path)\n",
        "    print(f\"Checking the parent directory: {parent_dir}\")\n",
        "    if os.path.exists(parent_dir):\n",
        "        print(f\"Contents of {parent_dir}:\")\n",
        "        try:\n",
        "            for item in os.listdir(parent_dir):\n",
        "                print(item)\n",
        "        except Exception as e:\n",
        "            print(f\"Could not list directory contents: {e}\")\n",
        "    else:\n",
        "        print(f\"Error: The parent directory was NOT found at {parent_dir}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8CyOlmqUbsQU"
      },
      "source": [
        "Save out audio to make sure is correct"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VGKCpTKubt0w"
      },
      "outputs": [],
      "source": [
        "import soundfile as sf\n",
        "import datetime\n",
        "import os # Keep os for path operations\n",
        "\n",
        "# Ensure the save directory exists\n",
        "save_dir = getSavePath() + \"Misc/\"\n",
        "if not os.path.exists(save_dir):\n",
        "    os.makedirs(save_dir)\n",
        "    print(f\"Created directory: {save_dir}\")\n",
        "\n",
        "# Define the output filename with a timestamp\n",
        "output_filename = os.path.join(save_dir, f\"imported_audio_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}.wav\")\n",
        "\n",
        "# Assuming 'audio' is a tuple where the first element is the audio data (numpy array)\n",
        "# and the second is the sample rate (sr)\n",
        "audio_data = audio[0]\n",
        "sample_rate = audio[1]\n",
        "\n",
        "# Save the audio data using soundfile\n",
        "sf.write(output_filename, audio_data, sample_rate)\n",
        "\n",
        "print(f\"Audio saved successfully to: {output_filename}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y-4i7J4zc9eu"
      },
      "source": [
        "Apply preprocessing to make spectrograms"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "twMkFDCuc_0v"
      },
      "outputs": [],
      "source": [
        "# Assuming 'audio' is a tuple from importAudio (audio_data, sr)\n",
        "# Pass only the audio data (the first element) to preprocessAudio\n",
        "preprocessed_mel, preprocessed_stft = preprocessAudio(audio[0])\n",
        "\n",
        "print(f\"Shape of preprocessed_mel after preprocessAudio: {preprocessed_mel.shape}\")\n",
        "print(f\"Shape of preprocessed_stft after preprocessAudio: {preprocessed_stft.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HS4dSa4dTTNxE"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import datetime # Ensure datetime is imported if not already\n",
        "import numpy as np # Ensure numpy is imported\n",
        "\n",
        "def print_and_save_spectrums(mel_spectrogram, stft_spectrogram, sr, save_path_prefix=(\"spectrum_output_\" + str(datetime.datetime.now())).replace(\" \", \"_\")):\n",
        "\n",
        "    # Let's calculate the time per column based on the STFT/Mel settings\n",
        "    # From STFTSpectrogram and MelSpectrogram: hopSize = 256\n",
        "    hopSize = 256 # Define hopSize here for use in calculating time axis\n",
        "    time_per_column = hopSize / sr\n",
        "\n",
        "    # Number of columns per second\n",
        "    columns_per_second = int(1 / time_per_column)\n",
        "\n",
        "    # Get the total duration in seconds based on the number of columns\n",
        "    total_duration_seconds = mel_spectrogram.shape[1] * time_per_column\n",
        "\n",
        "    print(f\"Total duration of spectrograms: {total_duration_seconds:.2f} seconds\")\n",
        "    print(f\"Columns per second: {columns_per_second}\")\n",
        "    print(f\"Shape of Mel spectrogram: {mel_spectrogram.shape}\")\n",
        "    print(f\"Shape of STFT spectrogram: {stft_spectrogram.shape}\")\n",
        "\n",
        "    # Ensure save directory exists\n",
        "    # Modified line to ensure save directory exists correctly within the mounted Google Drive\n",
        "    save_dir = os.path.dirname(getSavePath() + \"Spectrograms/\" + str(datetime.datetime.now()) + \"/\" + save_path_prefix)\n",
        "\n",
        "    if save_dir and not os.path.exists(save_dir):\n",
        "        os.makedirs(save_dir)\n",
        "        print(f\"Created directory: {save_dir}\")\n",
        "\n",
        "\n",
        "    for i in range(int(total_duration_seconds)):\n",
        "        start_col = i * columns_per_second\n",
        "        end_col = (i + 1) * columns_per_second\n",
        "\n",
        "        # Handle the last segment which might be less than 1 second\n",
        "        if start_col >= mel_spectrogram.shape[1]:\n",
        "            break\n",
        "        end_col = min(end_col, mel_spectrogram.shape[1])\n",
        "\n",
        "        print(f\"Processing time interval: {i}s to {i+1}s (columns {start_col} to {end_col})\")\n",
        "\n",
        "        mel_interval = mel_spectrogram[:, start_col:end_col]\n",
        "        stft_interval = np.abs(stft_spectrogram[:, start_col:end_col]) # Take absolute for magnitude\n",
        "\n",
        "        # --- Print Mel Spectrogram ---\n",
        "        plt.figure(figsize=(10, 4))\n",
        "        # Convert Mel spectrogram to dB for visualization\n",
        "        mel_interval_db = librosa.power_to_db(mel_interval, ref=np.max)\n",
        "\n",
        "        # --- Diagnostic Prints ---\n",
        "        print(f\"  Mel interval dB shape: {mel_interval_db.shape}\")\n",
        "        print(f\"  Mel interval dB dtype: {mel_interval_db.dtype}\")\n",
        "        print(f\"  Mel interval dB min/max: {np.min(mel_interval_db):.2f} / {np.max(mel_interval_db):.2f}\")\n",
        "        # --- End Diagnostic Prints ---\n",
        "\n",
        "\n",
        "        try:\n",
        "            librosa.display.specshow(mel_interval_db, sr=sr, hop_length=hopSize, x_axis='time', y_axis='mel')\n",
        "        except ValueError as e:\n",
        "            print(f\"  Error plotting Mel spectrogram with specshow: {e}\")\n",
        "            print(\"  Attempting to plot with plt.imshow...\")\n",
        "            # --- Alternative plotting with imshow ---\n",
        "            # Note: imshow requires origin='lower' for typical spectrogram orientation\n",
        "            # and extent for correct axis labels\n",
        "            plt.imshow(mel_interval_db, aspect='auto', origin='lower',\n",
        "                       extent=[start_col * time_per_column, end_col * time_per_column, 0, mel_interval_db.shape[0]],\n",
        "                       cmap='viridis') # Using a common colormap\n",
        "            plt.ylabel('Mel Bins')\n",
        "            plt.xlabel('Time (s)')\n",
        "            # --- End Alternative plotting ---\n",
        "\n",
        "\n",
        "        plt.colorbar(format='%+2.0f dB')\n",
        "        plt.title(f'Mel Spectrogram ({i}s - {i+1}s)')\n",
        "        mel_filename = f\"{save_dir}/{save_path_prefix}_mel_{i}s.png\"\n",
        "        plt.savefig(mel_filename)\n",
        "        plt.close() # Close the plot to free memory\n",
        "        print(f\"Saved Mel spectrogram for interval {i}s - {i+1}s to {mel_filename}\")\n",
        "\n",
        "\n",
        "        # --- Print STFT Spectrogram ---\n",
        "        plt.figure(figsize=(10, 4))\n",
        "        # Use librosa.amplitude_to_db for better visualization of STFT magnitude\n",
        "        stft_interval_db = librosa.amplitude_to_db(stft_interval, ref=np.max) # Convert STFT to dB for consistency\n",
        "\n",
        "        # --- Diagnostic Prints ---\n",
        "        print(f\"  STFT interval dB shape: {stft_interval_db.shape}\")\n",
        "        print(f\"  STFT interval dB dtype: {stft_interval_db.dtype}\")\n",
        "        print(f\"  STFT interval dB min/max: {np.min(stft_interval_db):.2f} / {np.max(stft_interval_db):.2f}\")\n",
        "        # --- End Diagnostic Prints ---\n",
        "\n",
        "        try:\n",
        "             librosa.display.specshow(stft_interval_db, sr=sr, hop_length=hopSize, x_axis='time', y_axis='log')\n",
        "        except ValueError as e:\n",
        "             print(f\"  Error plotting STFT spectrogram with specshow: {e}\")\n",
        "             print(\"  Attempting to plot with plt.imshow...\")\n",
        "             # --- Alternative plotting with imshow ---\n",
        "             plt.imshow(stft_interval_db, aspect='auto', origin='lower',\n",
        "                       extent=[start_col * time_per_column, end_col * time_per_column, 0, stft_interval_db.shape[0]],\n",
        "                       cmap='viridis')\n",
        "             plt.ylabel('STFT Bins')\n",
        "             plt.xlabel('Time (s)')\n",
        "             # --- End Alternative plotting ---\n",
        "\n",
        "\n",
        "        plt.colorbar(format='%+2.0f dB')\n",
        "        plt.title(f'STFT Spectrogram ({i}s - {i+1}s)')\n",
        "        stft_filename = f\"{save_dir}/{save_path_prefix}_stft_{i}s.png\"\n",
        "        plt.savefig(stft_filename)\n",
        "        plt.close() # Close the plot\n",
        "        print(f\"Saved STFT spectrogram for interval {i}s - {i+1}s to {stft_filename}\")\n",
        "\n",
        "# Example usage (assuming audio and sr are loaded and preprocessAudio is run)\n",
        "# audio, sr_loaded = importAudio(getFilePath())\n",
        "# mel_spec, stft_spec = preprocessAudio(audio)\n",
        "# print_and_save_spectrums(mel_spec, stft_spec, sr_loaded, save_path_prefix='audio_example') # Example save path"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kwRrzW8zfG5p"
      },
      "source": [
        "Print spectrograms"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "3ksRdS8SfJCw"
      },
      "outputs": [],
      "source": [
        "print_and_save_spectrums(preprocessed_mel, preprocessed_stft, getSR())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bACW-lrpD9xo"
      },
      "source": [
        "Random model testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g5ZHqNeIEb0o"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "maskA, maskB = predictMasks(preprocessed_mel, preprocessed_stft)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QqY4029QOuqw"
      },
      "source": [
        "Try using output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wTqIohSAOxUI"
      },
      "outputs": [],
      "source": [
        "import soundfile as sf\n",
        "import datetime\n",
        "import os # Keep os for path operations\n",
        "\n",
        "audioA = reconstructSingleVoice(getFilePath(), maskA)\n",
        "audioB = reconstructSingleVoice(getFilePath(), maskB)\n",
        "\n",
        "def saveAudioFile(file, path):\n",
        "    path = path + str(datetime.datetime.now())\n",
        "    if not os.path.exists(path):\n",
        "        os.makedirs(path)\n",
        "        print(f\"Created directory: {path}\")\n",
        "\n",
        "    # Define the output filename with a timestamp\n",
        "    output_filename = os.path.join(path, f\"exported_audio_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}.wav\")\n",
        "\n",
        "    audio_data = file\n",
        "    sample_rate = getSR()\n",
        "\n",
        "    # Save the audio data using soundfile\n",
        "    sf.write(output_filename, audio_data, sample_rate)\n",
        "\n",
        "    print(f\"Audio saved successfully to: {output_filename}\")\n",
        "\n",
        "\n",
        "saveAudioFile(audioA, getOutputAPath())\n",
        "saveAudioFile(audioB, getOutputBPath())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A-l38o9HV73t"
      },
      "source": [
        "All blank model running"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kSqgwSNEWAh7"
      },
      "outputs": [],
      "source": [
        "def saveAudioFile(file, path):\n",
        "    path = path + str(datetime.datetime.now())\n",
        "    if not os.path.exists(path):\n",
        "        os.makedirs(path)\n",
        "        print(f\"Created directory: {path}\")\n",
        "\n",
        "    # Define the output filename with a timestamp\n",
        "    output_filename = os.path.join(path, f\"exported_audio_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}.wav\")\n",
        "\n",
        "    audio_data = file\n",
        "    sample_rate = getSR()\n",
        "\n",
        "    # Save the audio data using soundfile\n",
        "    sf.write(output_filename, audio_data, sample_rate)\n",
        "\n",
        "    print(f\"Audio saved successfully to: {output_filename}\")\n",
        "\n",
        "audio = importAudio(getFilePath())\n",
        "preprocessed_mel, preprocessed_stft = preprocessAudio(audio[0])\n",
        "maskA, maskB = predictMasks(preprocessed_mel, preprocessed_stft)\n",
        "audioA = reconstructSingleVoice(getFilePath(), maskA)\n",
        "audioB = reconstructSingleVoice(getFilePath(), maskB)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GIVhqEizWVWI"
      },
      "source": [
        "#Training Working\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mf7rB3JoWZXG"
      },
      "source": [
        "Import global Z score dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rfib4merXOW0"
      },
      "outputs": [],
      "source": [
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "\n",
        "dataset = SpectrogramDataset(melPath, stftPath, maskPathA, maskPathB)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "MMRknq_07iys"
      },
      "outputs": [],
      "source": [
        "computeZScoreMeanStd(dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rn11xM0nA5We"
      },
      "outputs": [],
      "source": [
        "\n",
        "runTraining()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v4qx-5SJ-54Z"
      },
      "outputs": [],
      "source": [
        "#check GPU performance\n",
        "\n",
        "!nvidia-smi\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N6ikNjSFU_yT"
      },
      "source": [
        "#Run the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hl6mMxWyVITG"
      },
      "outputs": [],
      "source": [
        "import soundfile as sf\n",
        "\n",
        "\n",
        "audio, _ = importAudio(getFilePath())  # Unpack the tuple\n",
        "\n",
        "\n",
        "\n",
        "audioMel, audioSTFT = preprocessAudio(audio)\n",
        "\n",
        "\n",
        "maskA, maskB = predictMasks(audioMel, audioSTFT)\n",
        "\n",
        "outputA = reconstructSingleVoice(getFilePath(), maskA)\n",
        "outputB = reconstructSingleVoice(getFilePath(), maskB)\n",
        "\n",
        "timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "\n",
        "sf.write(getOutputAPath() + f\"_{timestamp}.wav\", outputA, samplerate=getSR())\n",
        "sf.write(getOutputBPath() + f\"_{timestamp}.wav\", outputB, samplerate=getSR())\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WSu9nPChhi3p"
      },
      "outputs": [],
      "source": [
        "getFilePath()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BgHeB4h6eJar"
      },
      "source": [
        "Test Reconstruction to make sure training masks are correct"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RTkx4thaeNUB"
      },
      "outputs": [],
      "source": [
        "originalFile = \"/content/drive/My Drive/We Love Parth!!/Data Collection/Model Data Storage (DO NOT RENAME)/All Data/combined iwitw angadi melody iwitw barsotti harmony.wav\"\n",
        "testMaskPath = \"/content/drive/My Drive/We Love Parth!!/Data Collection/Model Data Storage (DO NOT RENAME)/All Data/mask_folderA/combined iwitw angadi melody iwitw barsotti harmony.npy\"\n",
        "\n",
        "\n",
        "\n",
        "mask = np.load(testMaskPath)\n",
        "\n",
        "reconstructedAudio = reconstructSingleVoice(originalFile, mask, sampleRate=getSR())\n",
        "\n",
        "timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "\n",
        "sf.write(\"/content/drive/My Drive/We Love Parth!!/Data Collection/Model Data Storage (DO NOT RENAME)/Misc/\" + f\"_{timestamp}.wav\", reconstructedAudio, samplerate=getSR())\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QqQ31BecJMny"
      },
      "source": [
        "#cache data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BD3eCoLhJOAJ"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import shutil\n",
        "from google.colab import auth\n",
        "from google.colab import drive\n",
        "import datetime\n",
        "\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "def cache_data_to_local(src_root, dst_root, subfolders):\n",
        "    os.makedirs(dst_root, exist_ok=True)\n",
        "\n",
        "    for folder in subfolders:\n",
        "        src_path = os.path.join(src_root, folder)\n",
        "        dst_path = os.path.join(dst_root, folder)\n",
        "        os.makedirs(dst_path, exist_ok=True)\n",
        "\n",
        "        files = sorted(f for f in os.listdir(src_path) if f.endswith('.npy'))\n",
        "        for f in files:\n",
        "            src_file = os.path.join(src_path, f)\n",
        "            dst_file = os.path.join(dst_path, f)\n",
        "            if not os.path.exists(dst_file):  # skip if already cached\n",
        "                shutil.copy2(src_file, dst_file)\n",
        "        print(f\"‚úÖ Cached {len(files)} files from {folder}\")\n",
        "\n",
        "# Example usage\n",
        "google_drive_root = \"/content/drive/My Drive/We Love Parth!!/Data Collection/Model Data Storage (DO NOT RENAME)/All Data\"\n",
        "local_cache_root = \"/content/cached_data\"\n",
        "\n",
        "os.makedirs(local_cache_root, exist_ok=True)\n",
        "\n",
        "subfolders = [\"mel_folder3\", \"stft_folder3\", \"mask_folderA3\", \"mask_folderB3\"]\n",
        "cache_data_to_local(google_drive_root, local_cache_root, subfolders)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nA2VfpRwtmuf"
      },
      "source": [
        "Create validation split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "irIzFNKatoGn"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import random\n",
        "import shutil\n",
        "\n",
        "# === Settings ===\n",
        "x = 100  # Number of validation sets to move\n",
        "\n",
        "base_path = \"/content/cached_data\"\n",
        "mel_folder      = os.path.join(base_path, \"mel_folder3\")\n",
        "stft_folder     = os.path.join(base_path, \"stft_folder3\")\n",
        "maskA_folder    = os.path.join(base_path, \"mask_folderA3\")\n",
        "maskB_folder    = os.path.join(base_path, \"mask_folderB3\")\n",
        "\n",
        "# Create validation folders\n",
        "val_mel      = mel_folder + \"_val\"\n",
        "val_stft     = stft_folder + \"_val\"\n",
        "val_maskA    = maskA_folder + \"_val\"\n",
        "val_maskB    = maskB_folder + \"_val\"\n",
        "\n",
        "for folder in [val_mel, val_stft, val_maskA, val_maskB]:\n",
        "    os.makedirs(folder, exist_ok=True)\n",
        "\n",
        "# === Pick random subset ===\n",
        "mel_files = sorted([f for f in os.listdir(mel_folder) if f.endswith(\".npy\")])\n",
        "val_files = random.sample(mel_files, min(x, len(mel_files)))\n",
        "\n",
        "# === Move matching sets ===\n",
        "for fname in val_files:\n",
        "    shutil.move(os.path.join(mel_folder, fname),   os.path.join(val_mel, fname))\n",
        "    shutil.move(os.path.join(stft_folder, fname),  os.path.join(val_stft, fname))\n",
        "    shutil.move(os.path.join(maskA_folder, fname), os.path.join(val_maskA, fname))\n",
        "    shutil.move(os.path.join(maskB_folder, fname), os.path.join(val_maskB, fname))\n",
        "\n",
        "print(f\"‚úÖ Moved {len(val_files)} sets to validation folders.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SgZi_Ig_7n3l"
      },
      "source": [
        "#Test Masks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "602OafFH7plm"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "from google.colab import auth\n",
        "from google.colab import drive\n",
        "import datetime\n",
        "\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "# ==== MODIFY THESE PATHS ====\n",
        "mel_path   = \"/content/drive/My Drive/We Love Parth!!/Data Collection/Model Data Storage (DO NOT RENAME)/All Data/mel_folder3/combined iwitw albright melody iwitw albright harmony.npy\"\n",
        "stft_path  = \"/content/drive/My Drive/We Love Parth!!/Data Collection/Model Data Storage (DO NOT RENAME)/All Data/stft_folder3/combined iwitw albright melody iwitw albright harmony.npy\"\n",
        "maskA_path = \"/content/drive/My Drive/We Love Parth!!/Data Collection/Model Data Storage (DO NOT RENAME)/All Data/mask_folderA3/combined iwitw albright melody iwitw albright harmony.npy\"\n",
        "maskB_path = \"/content/drive/My Drive/We Love Parth!!/Data Collection/Model Data Storage (DO NOT RENAME)/All Data/mask_folderB3/combined iwitw albright melody iwitw albright harmony.npy\"\n",
        "\n",
        "model_path = getModelSavePath()  # or replace with .pt path if saved manually\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# ==== LOAD DATA ====\n",
        "mel = np.load(mel_path)\n",
        "stft = np.load(stft_path)\n",
        "maskA = np.load(maskA_path)\n",
        "maskB = np.load(maskB_path)\n",
        "\n",
        "# Truncate or pad to match time dims if needed\n",
        "min_T = min(mel.shape[1], stft.shape[1], maskA.shape[1], maskB.shape[1])\n",
        "mel = mel[:, :min_T]\n",
        "stft = stft[:, :min_T]\n",
        "maskA = maskA[:, :min_T]\n",
        "maskB = maskB[:, :min_T]\n",
        "\n",
        "# Convert to torch tensors\n",
        "mel_tensor = torch.tensor(mel).unsqueeze(0).unsqueeze(0).float().to(device)  # (1, 1, 128, T)\n",
        "stft_tensor = torch.tensor(stft).unsqueeze(0).unsqueeze(0).float().to(device)  # (1, 1, 513, T)\n",
        "\n",
        "# ==== LOAD MODEL ====\n",
        "model = DualUNetBLSTM()\n",
        "model.load_state_dict(torch.load(model_path, map_location=device))\n",
        "model.to(device)\n",
        "model.eval()\n",
        "\n",
        "# ==== INFERENCE ====\n",
        "with torch.no_grad():\n",
        "    pred = model(mel_tensor, stft_tensor)\n",
        "    predA = pred[0, 0].cpu().numpy()\n",
        "    predB = pred[0, 1].cpu().numpy()\n",
        "\n",
        "# ==== PLOT ====\n",
        "fig, axs = plt.subplots(2, 2, figsize=(14, 8))\n",
        "\n",
        "axs[0, 0].imshow(maskA, aspect='auto', origin='lower')\n",
        "axs[0, 0].set_title(\"Ground Truth Mask A\")\n",
        "\n",
        "axs[0, 1].imshow(predA, aspect='auto', origin='lower')\n",
        "axs[0, 1].set_title(\"Predicted Mask A\")\n",
        "\n",
        "axs[1, 0].imshow(maskB, aspect='auto', origin='lower')\n",
        "axs[1, 0].set_title(\"Ground Truth Mask B\")\n",
        "\n",
        "axs[1, 1].imshow(predB, aspect='auto', origin='lower')\n",
        "axs[1, 1].set_title(\"Predicted Mask B\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Fvy01f7dFLL"
      },
      "source": [
        "#bottom"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#output testing"
      ],
      "metadata": {
        "id": "H-0zATpGBpLh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install mir_eval\n"
      ],
      "metadata": {
        "id": "GcdRt0KHBqzl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from mir_eval.separation import bss_eval_sources\n",
        "\n",
        "def compute_mask_accuracy(predA, predB, maskA, maskB, threshold=0.5):\n",
        "    predA_bin = (predA > threshold).astype(np.float32)\n",
        "    predB_bin = (predB > threshold).astype(np.float32)\n",
        "    maskA_bin = (maskA > threshold).astype(np.float32)\n",
        "    maskB_bin = (maskB > threshold).astype(np.float32)\n",
        "\n",
        "    correctA = np.sum(predA_bin == maskA_bin)\n",
        "    correctB = np.sum(predB_bin == maskB_bin)\n",
        "    total = maskA.size + maskB.size\n",
        "\n",
        "    return (correctA + correctB) / total\n",
        "\n",
        "\n",
        "def evaluate_on_loader(val_loader):\n",
        "    sdr_list, sir_list, sar_list = [], [], []\n",
        "    acc_list = []\n",
        "\n",
        "    for batch in val_loader:\n",
        "        if batch is None:\n",
        "            continue\n",
        "        mel, stft, maskA, maskB = batch\n",
        "\n",
        "        for i in range(mel.size(0)):\n",
        "\n",
        "            if i is None:\n",
        "                continue  # Skip batches with only bad data\n",
        "\n",
        "            mel_i = mel[i].squeeze().cpu().numpy()\n",
        "            stft_i = stft[i].squeeze().cpu().numpy()\n",
        "            maskA_i = maskA[i].squeeze().cpu().numpy()\n",
        "            maskB_i = maskB[i].squeeze().cpu().numpy()\n",
        "\n",
        "            try:\n",
        "                predA, predB = predictMasks(mel_i, stft_i)\n",
        "\n",
        "                # Compute reconstructions\n",
        "                estA = predA * stft_i\n",
        "                estB = predB * stft_i\n",
        "                trueA = maskA_i * stft_i\n",
        "                trueB = maskB_i * stft_i\n",
        "\n",
        "                ref = np.stack([trueA.flatten(), trueB.flatten()])\n",
        "                est = np.stack([estA.flatten(), estB.flatten()])\n",
        "\n",
        "                sdr, sir, sar, _ = bss_eval_sources(ref, est)\n",
        "\n",
        "                sdr_list.append(np.mean(sdr))\n",
        "                sir_list.append(np.mean(sir))\n",
        "                sar_list.append(np.mean(sar))\n",
        "\n",
        "                # Compute mask accuracy\n",
        "                acc = compute_mask_accuracy(predA.numpy(), predB.numpy(), maskA_i, maskB_i)\n",
        "                acc_list.append(acc)\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"‚ö†Ô∏è Skipped sample due to: {e}\")\n",
        "\n",
        "    print(\"\\n‚úÖ Evaluation complete.\")\n",
        "    print(f\"SDR: {np.mean(sdr_list):.4f}\")\n",
        "    print(f\"SIR: {np.mean(sir_list):.4f}\")\n",
        "    print(f\"SAR: {np.mean(sar_list):.4f}\")\n",
        "    print(f\"Mask Accuracy: {np.mean(acc_list):.4f}\")\n",
        "\n",
        "    return np.mean(sdr_list), np.mean(sir_list), np.mean(sar_list), np.mean(acc_list)\n",
        "\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "\n",
        "val_dataset = SpectrogramDataset(\n",
        "    melFolder='/content/cached_data/mel_folder3_val',\n",
        "    stftFolder='/content/cached_data/stft_folder3_val',\n",
        "    maskFolderA='/content/cached_data/mask_folderA3_val',\n",
        "    maskFolderB='/content/cached_data/mask_folderB3_val',\n",
        "    max_T=1024  # or whatever your default is\n",
        ")\n",
        "\n",
        "def safe_collate(batch):\n",
        "    batch = [b for b in batch if b is not None]\n",
        "    if len(batch) == 0:\n",
        "        return None\n",
        "    return torch.utils.data.default_collate(batch)\n",
        "\n",
        "\n",
        "val_loader = DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size=1,\n",
        "    shuffle=False,\n",
        "    collate_fn=safe_collate\n",
        ")\n",
        "\n",
        "\n",
        "evaluate_on_loader(val_loader)\n"
      ],
      "metadata": {
        "id": "YvtmGu-JBs9z"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}